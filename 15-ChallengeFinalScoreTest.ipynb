{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV, StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import Imputer, StandardScaler, MinMaxScaler, RobustScaler, Normalizer, LabelEncoder, LabelBinarizer\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.ensemble  import RandomForestClassifier \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "warnings.filterwarnings(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(action='ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings(action='ignore', category=UndefinedMetricWarning)\n",
    "\n",
    "data_training = pd.read_csv('input_train.csv')\n",
    "label = pd.read_csv('challenge_output_data_training_file_prediction_of_transaction_claims_status.csv', ';')\n",
    "data_test = pd.read_csv('input_test.csv')\n",
    "all_data_training = data_training.merge(label, left_on='ID', right_on='ID', how='inner').copy()\n",
    "claim_order = sorted(all_data_training['CLAIM_TYPE'].unique())\n",
    "\n",
    "\n",
    "# To improve the Unbalanced all_data_trainingset we can delete the missing values for CLAIM_TYPE= '-'\n",
    "all_data_training = all_data_training.drop(all_data_training[(all_data_training['CLAIM_TYPE']=='-') & (all_data_training['BUYER_BIRTHDAY_DATE'].isna()) ].index)\n",
    "all_data_training = all_data_training.drop(all_data_training[(all_data_training['CLAIM_TYPE']=='-') & (all_data_training['SELLER_SCORE_AVERAGE'].isna()) ].index)\n",
    "all_data_training = all_data_training.drop(all_data_training[(all_data_training['CLAIM_TYPE']=='-') & (all_data_training['SHIPPING_MODE'].isna()) ].index)\n",
    "all_data_training = all_data_training.drop(all_data_training[(all_data_training['CLAIM_TYPE']=='-') & (all_data_training['SHIPPING_PRICE'].isna()) ].index)\n",
    "all_data_training = all_data_training.drop(all_data_training[(all_data_training['CLAIM_TYPE']=='-') & (all_data_training['PRICECLUB_STATUS'].isna()) ].index)\n",
    "all_data_training = all_data_training.drop(all_data_training[(all_data_training['CLAIM_TYPE']=='-') & (all_data_training['SELLER_SCORE_COUNT'].isna()) ].index)\n",
    "\n",
    "# Filter Unwanted Outliers\n",
    "all_data_training = all_data_training.drop(all_data_training[(all_data_training['BUYER_BIRTHDAY_DATE']<1940) & (all_data_training['BUYER_BIRTHDAY_DATE']>=0)].index)\n",
    "all_data_training = all_data_training.drop(all_data_training[(all_data_training['BUYER_BIRTHDAY_DATE']>=2000) ].index)\n",
    "all_data_training = all_data_training.drop(all_data_training[(all_data_training['SELLER_SCORE_AVERAGE']<40) & (all_data_training['SELLER_SCORE_AVERAGE']>=0)].index)\n",
    "\n",
    "#Encoding\n",
    "factor = pd.factorize(all_data_training['CLAIM_TYPE'])\n",
    "target_number=factor[0]\n",
    "target_label = factor[1]\n",
    "target_vectoriser=np.vectorize(dict(zip(range(8),target_label)).get)\n",
    "lb=LabelBinarizer()\n",
    "all_data_training = all_data_training.drop([\"CLAIM_TYPE\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_traitement(data):\n",
    "\n",
    "    #Fixing Structural Errors\n",
    "    temp = data['BUYING_DATE'].str.split('/')\n",
    "    data['BUYING_DATE'] = temp.str[1] + temp.str[0].str.zfill(2)\n",
    "    data['BUYING_DATE'] = data['BUYING_DATE'].astype('int32')\n",
    "\n",
    "    # Numerical feature \n",
    "    data['REGISTRATION_DATE']=data['REGISTRATION_DATE'].fillna(-1).values\n",
    "    data['BUYER_BIRTHDAY_DATE']=data['BUYER_BIRTHDAY_DATE'].fillna(-1).values\n",
    "    data['BUYING_DATE']=data['BUYING_DATE'].fillna(-1).values\n",
    "    data['SELLER_SCORE_AVERAGE']=data['SELLER_SCORE_AVERAGE'].fillna(-1).values \n",
    "\n",
    "    # WARRANTIES_PRICE: There is not missing value but NON_WARRANTIES\n",
    "    data['WARRANTIES_PRICE']=data['WARRANTIES_PRICE'].fillna('NON_WARRANTIES').values \n",
    "\n",
    "    # Categorical Features\n",
    "    data = data.fillna('MISSING')\n",
    "\n",
    "    # Data Type Conversion\n",
    "    data['BUYER_BIRTHDAY_DATE']=data['BUYER_BIRTHDAY_DATE'].astype(np.int16)\n",
    "    data['SELLER_SCORE_AVERAGE']=data['SELLER_SCORE_AVERAGE'].astype(np.int16)\n",
    "    data['REGISTRATION_DATE']=data['REGISTRATION_DATE'].astype(np.int16)\n",
    "    data['BUYER_DEPARTMENT']=data['BUYER_DEPARTMENT'].astype(np.int16)\n",
    "    data['SELLER_DEPARTMENT']=data['SELLER_DEPARTMENT'].astype(np.int16)\n",
    "    data['CARD_PAYMENT']=data['CARD_PAYMENT'].astype(np.bool)\n",
    "    data['COUPON_PAYMENT']=data['COUPON_PAYMENT'].astype(np.bool)\n",
    "    data['RSP_PAYMENT']=data['RSP_PAYMENT'].astype(np.bool)\n",
    "    data['WALLET_PAYMENT']=data['WALLET_PAYMENT'].astype(np.bool)\n",
    "\n",
    "    #Sparse Classes\n",
    "    top10=data['SELLER_COUNTRY'].value_counts().head(10).index\n",
    "    data.loc[data['SELLER_COUNTRY'].isin(top10)==False,'SELLER_COUNTRY'] = 'OTHERS'\n",
    "\n",
    "    data.loc[data['SELLER_DEPARTMENT']>98,'SELLER_DEPARTMENT'] = 'OTHERS'\n",
    "    data.loc[data['BUYER_DEPARTMENT']>97,'BUYER_DEPARTMENT'] = 'OTHERS'\n",
    "    \n",
    "    top5=data['PRODUCT_TYPE'].value_counts().head(5).index\n",
    "    data.loc[data['PRODUCT_TYPE'].isin(top5)==False,'PRODUCT_TYPE'] = 'OTHERS'\n",
    "\n",
    "    ####### Nominal #######\n",
    "    data = pd.concat([data,\n",
    "                                    pd.get_dummies(data['SHIPPING_MODE'], prefix='SHIPPING_MODE'),\n",
    "                                    pd.get_dummies(data['SELLER_COUNTRY'], prefix='SELLER_COUNTRY'),\n",
    "                                    pd.get_dummies(data['PRODUCT_TYPE'], prefix='PRODUCT_TYPE'),\n",
    "                                    pd.get_dummies(data['PRODUCT_FAMILY'], prefix='PRODUCT_FAMILY'),\n",
    "                                    pd.get_dummies(data['BUYER_DEPARTMENT'], prefix='BUYER_DEPARTMENT'),\n",
    "                                    pd.get_dummies(data['SELLER_DEPARTMENT'], prefix='SELLER_DEPARTMENT')\n",
    "                               ],\n",
    "                              axis=1)\n",
    "\n",
    "    #######Ordinal #######\n",
    "    data['PRICECLUB_STATUS'] = data['PRICECLUB_STATUS'].map( {'MISSING':0,'UNSUBSCRIBED':1, 'REGULAR': 2, 'SILVER':3, 'GOLD':4, 'PLATINUM':5}).astype(np.int16)\n",
    "    data['SHIPPING_PRICE'] = data['SHIPPING_PRICE'].map( {'MISSING':0,'<1':1, '1<5': 2, '5<10':3, '10<20':4, '>20':5}).astype(np.int16)\n",
    "    data['WARRANTIES_PRICE'] = data['WARRANTIES_PRICE'].map( {'NON_WARRANTIES':0, '<5':1, '5<20': 2, '20<50':3, '50<100':4, '100<500':5}).astype(np.int16)\n",
    "    data['PURCHASE_COUNT'] = data['PURCHASE_COUNT'].map( {'<5':1, '5<20': 2, '20<50':3, '50<100':4, '100<500':5, '>500':6}).astype(np.int16)\n",
    "    data['SELLER_SCORE_COUNT'] = data['SELLER_SCORE_COUNT'].map( {'MISSING':0,'<100':1, '100<1000': 2, '1000<10000':3, '10000<100000':4, '100000<1000000':5}).astype(np.int16)\n",
    "    data['ITEM_PRICE'] = data['ITEM_PRICE'].map( {'<10':0, '10<20': 1, '20<50':2, '50<100':3, '100<500':4, '500<1000':5, '1000<5000':6, '>5000':7}).astype(np.int16)\n",
    "\n",
    "    #Remove Unused Features\n",
    "    # REMOVAL OF UNUSEFUL FEATURES\n",
    "    data = data.drop([\"ID\"], axis=1)\n",
    "    data = data.drop(['WARRANTIES_FLG'],  axis=1)\n",
    "\n",
    "    # REMOVAL OF FEATURES THAT WERE ENCODED\n",
    "    data = data.drop(['SHIPPING_MODE'],  axis=1)\n",
    "    data = data.drop(['SELLER_COUNTRY'],  axis=1)\n",
    "    data = data.drop(['PRODUCT_FAMILY'],  axis=1)\n",
    "    data = data.drop(['PRODUCT_TYPE'],  axis=1)\n",
    "    data = data.drop(['BUYER_DEPARTMENT'],  axis=1)\n",
    "    data = data.drop(['SELLER_DEPARTMENT'],  axis=1)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_classifier(classifier, data_training, data_validate):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(data_training.values, target_number, test_size=0.20, random_state=0, stratify=target_number)\n",
    "    w=compute_sample_weight(class_weight='balanced', y=y_train) \n",
    "    imputer = Imputer(strategy='mean', missing_values=-1) \n",
    "\n",
    "    #fill of NaN\n",
    "    imputer.fit(X_train)\n",
    "    X_train_imputed = imputer.transform(X_train)\n",
    "    X_test_imputed = imputer.transform(X_test)\n",
    "    \n",
    "    \n",
    "    classifier.fit(X_train_imputed, y_train, sample_weight=w)\n",
    "    y_train_predicted = classifier.predict(X_train_imputed)\n",
    "    y_test_predicted = classifier.predict(X_test_imputed)\n",
    "\n",
    "    y_train_predicted_label = target_vectoriser(y_train_predicted)\n",
    "    y_test_predicted_label = target_vectoriser(y_test_predicted)\n",
    "    y_train_label = target_vectoriser(y_train)\n",
    "    y_test_label = target_vectoriser(y_test)\n",
    "    print('TRAIN')\n",
    "    print('f1_score:      ',metrics.f1_score(y_train, y_train_predicted, average='weighted'), '\\nroc_auc_score: ', metrics.roc_auc_score(lb.fit_transform(y_train), lb.fit_transform(y_train_predicted), average='weighted'), '\\n')\n",
    "    print(classification_report(y_train_label, y_train_predicted_label))\n",
    "    print('TEST')\n",
    "    print('f1_score:      ',metrics.f1_score(y_test, y_test_predicted, average='weighted'), '\\nroc_auc_score: ', metrics.roc_auc_score(lb.fit_transform(y_test), lb.fit_transform(y_test_predicted), average='weighted'), '\\n')\n",
    "    print(classification_report(y_test_label, y_test_predicted_label))\n",
    "\n",
    "    X_validate_imputed = imputer.transform(data_validate)\n",
    "    score = classifier.predict(X_validate_imputed)\n",
    "    df =  pd.DataFrame( columns=['ID', 'CLAIM_TYPE'])\n",
    "    df['ID']=data_test['ID']\n",
    "    df['CLAIM_TYPE']=target_vectoriser(score)\n",
    "\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_t = pre_traitement(all_data_training)\n",
    "data_v = pre_traitement(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\n",
      "f1_score:       0.9822475378248254 \n",
      "roc_auc_score:  0.9857010783109489 \n",
      "\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "                       -       1.00      0.99      1.00     12233\n",
      "                 DAMAGED       0.99      0.96      0.98      4638\n",
      "               DIFFERENT       0.99      0.97      0.98      3342\n",
      "                    FAKE       0.07      1.00      0.14        98\n",
      "            NOT_RECEIVED       1.00      0.95      0.97     11321\n",
      "SELLER_CANCEL_POSTERIORI       1.00      0.98      0.99     10791\n",
      "               UNDEFINED       0.98      0.97      0.97      3159\n",
      "              WITHDRAWAL       1.00      0.97      0.98      5462\n",
      "\n",
      "             avg / total       0.99      0.97      0.98     51044\n",
      "\n",
      "TEST\n",
      "f1_score:       0.47115683277337045 \n",
      "roc_auc_score:  0.6874489982591161 \n",
      "\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "                       -       0.59      0.87      0.70      3059\n",
      "                 DAMAGED       0.36      0.20      0.26      1159\n",
      "               DIFFERENT       0.42      0.14      0.21       836\n",
      "                    FAKE       0.01      0.42      0.02        24\n",
      "            NOT_RECEIVED       0.52      0.48      0.50      2831\n",
      "SELLER_CANCEL_POSTERIORI       0.51      0.51      0.51      2698\n",
      "               UNDEFINED       0.47      0.20      0.28       790\n",
      "              WITHDRAWAL       0.42      0.21      0.28      1365\n",
      "\n",
      "             avg / total       0.50      0.49      0.47     12762\n",
      "\n",
      "Wall time: 2min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "classifier = RandomForestClassifier(n_estimators=200,max_depth=50, class_weight=\"balanced_subsample\", random_state=0)\n",
    "df=run_classifier(classifier, data_t, data_v)\n",
    "df.to_csv(\"data_v_RF.csv\", index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\n",
      "f1_score:       0.14853978578108878 \n",
      "roc_auc_score:  0.5255049045825522 \n",
      "\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "                       -       0.32      0.53      0.40     12233\n",
      "                 DAMAGED       0.10      0.02      0.03      4638\n",
      "               DIFFERENT       0.11      0.06      0.08      3342\n",
      "                    FAKE       0.00      0.56      0.01        98\n",
      "            NOT_RECEIVED       0.24      0.04      0.07     11321\n",
      "SELLER_CANCEL_POSTERIORI       0.23      0.05      0.08     10791\n",
      "               UNDEFINED       0.08      0.07      0.07      3159\n",
      "              WITHDRAWAL       0.17      0.04      0.06      5462\n",
      "\n",
      "             avg / total       0.22      0.16      0.15     51044\n",
      "\n",
      "TEST\n",
      "f1_score:       0.14104471719014652 \n",
      "roc_auc_score:  0.5208552036561523 \n",
      "\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "                       -       0.31      0.52      0.39      3059\n",
      "                 DAMAGED       0.11      0.02      0.03      1159\n",
      "               DIFFERENT       0.09      0.05      0.07       836\n",
      "                    FAKE       0.00      0.42      0.00        24\n",
      "            NOT_RECEIVED       0.22      0.04      0.07      2831\n",
      "SELLER_CANCEL_POSTERIORI       0.20      0.04      0.07      2698\n",
      "               UNDEFINED       0.09      0.06      0.08       790\n",
      "              WITHDRAWAL       0.15      0.04      0.06      1365\n",
      "\n",
      "             avg / total       0.20      0.16      0.14     12762\n",
      "\n",
      "Wall time: 6.76 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "classifier =  LogisticRegression(solver='lbfgs', multi_class='multinomial')\n",
    "df=run_classifier(classifier, data_t, data_v)\n",
    "df.to_csv(\"data_v_LR.csv\", index=False, sep=\";\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
